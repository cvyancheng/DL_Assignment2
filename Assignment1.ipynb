{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064459f4-a504-46b9-9cba-2c9148cc24c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Deep Learning Assignment 1\n",
    "Author: 林彥成, P88101029, NCKU\n",
    "Date: 21 Mar, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dcd3b4-f35f-4d74-86b7-da5173ac1d77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## General description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5c2a9-10d0-4024-ab65-ba8078073e06",
   "metadata": {},
   "source": [
    "\n",
    "Please download the dataset from Moodle. Regarding the detailed information of the dataset, please see the readme in the dataset.\n",
    "\n",
    "Please build an \"Image Classification Pipeline\" based on the following:\n",
    "\n",
    "* Reading images to an array\n",
    "* Feature extraction (transform the image into a fixed-length feature vector)\n",
    "* Apply any classifier to verify the performance.\n",
    "\n",
    "It is suggested to use OpenCV lib to extract features and use a linear classifier (Perceptron) to predict the label. You can find perception easily from GitHub, like\n",
    "\n",
    "* Perceptron: https://github.com/Vercaca/Perceptron/blob/master/perceptron.py \n",
    "* Reference: https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L03%20Linear%20Classifiers.pdf\n",
    "\n",
    "Using any open-source perception is acceptable, and you must indicate which GitHub you adopted.\n",
    "\n",
    "You should at least adopt one image feature extractor to transform the pixel domain into another feature domain. \n",
    "\n",
    "The recommended image feature extraction methods (but not limited to) can be found as follows:\n",
    "\n",
    "* Global color histogram [ref: https://en.wikipedia.org/wiki/Color_histogram]\n",
    "* HoG [ref: https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients]\n",
    "* BoVW [ref: https://medium.com/analytics-vidhya/bag-of-visual-words-bag-of-features-9a2f7aec7866]\n",
    "\n",
    "Note that you should train a model based on the training set and evaluate the performance by validation and testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190e5e6-ff67-4b07-9c73-c1505b41abc4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b848dd-42cb-4778-bfc1-997d2676e9ca",
   "metadata": {},
   "source": [
    "Your code needs to meet the functionality below:\n",
    "* Any classifier is acceptable. For example, SVM, perception, linear classifier, or even NN classifier. In this assignment, you should choose at least three different classifiers to evaluate the performance based on our dataset.\n",
    "* Image feature extraction (can be any existing package)\n",
    "* For the training phase, your algorithm can be used to train a model based on the extracted features of the training samples.\n",
    "* For the testing phase, your code should be able to effectively classify the extracted feature of the test sample.\n",
    "* At least one ensemble learning-based classifier should be used, such as AdaBoost, Xgboost, Lightgbm, Catboost, etc.,\n",
    "* ReadMe (markdown format) indicates how to train a model and evaluate performance. \n",
    "* Complete source codes (Link of Colab or GitHub)\n",
    "* Word/PDF report to show that\n",
    "    * GitHub/Colab link\n",
    "    * What model you used\n",
    "    * The curves of the training accuracy and validation accuracy\n",
    "    * The predicted result on the validation/testing  set (evaluated in top-1 accuracy and top-5 accuracy, DO NOT use any existing package/toolbox)\n",
    "* Performance comparison among three classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e629c271-a865-4401-982b-c56fb9c9af2d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeafb50-adac-4f45-9569-6aa91455c972",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 環境資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4d3b85-2e66-4bfb-8885-97e4f501f812",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 21 17:22:45 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.57       Driver Version: 515.57       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:06:00.0  On |                  N/A |\n",
      "| 25%   43C    P5    24W / 125W |    918MiB /  6144MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2802      G   /usr/libexec/Xorg                 358MiB |\n",
      "|    0   N/A  N/A      2956      G   /usr/bin/gnome-shell              240MiB |\n",
      "|    0   N/A  N/A   2147404      G   /usr/lib64/firefox/firefox        316MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5836528a-8338-4088-a3da-ec3c96070a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee4ad1-419f-4f72-a583-39abaad1fd32",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 讀取圖像數據並準備數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "057ea061-2e00-4b2a-85df-82c835165437",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images.zip has been extracted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "dataset_path = './dataset'\n",
    "if not os.path.exists(dataset_path):\n",
    "    with zipfile.ZipFile('./images.zip', 'r') as zf:\n",
    "        zf.extractall(path=dataset_path)\n",
    "else:\n",
    "    print('The images.zip has been extracted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f7c07-940e-40b4-b1d6-bd6c78b3e294",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 載入所需套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "feed2790-5899-4d3c-81f0-b3a8529f65fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ddda61-6b9f-4b85-962d-e35bcc89e212",
   "metadata": {},
   "source": [
    "# 圖像特徵提取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e1858-b05b-4d2d-9337-b957e92e7625",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 讀取資料與建立feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad0b3d9-b07d-4a02-911f-6f8df58e3886",
   "metadata": {},
   "source": [
    "* 首先利用pytorch中的dataset模組建立自己的dataset\n",
    "* 接著將三個檔案分別包裝成dataset格式之後利用pytorch的dataloader載入以便後續使用\n",
    "* 最後建立一個feature extractor(在此次作業中我是使用pytorch提供的預訓練resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aed0230-aeb2-43b5-aa24-c8f1265d0e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定義自己的dataset\n",
    "class dataset_C0(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, image_size=256):\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        self.data = [line.rstrip().split() for line in lines]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(image_size-1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.root_path = './dataset/'\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, image_label = self.data[idx]\n",
    "        img = Image.open(self.root_path+image_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img, int(image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# train set, val set, test set\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "train_data = dataset_C0(os.path.join(dataset_path,'train.txt'), IMG_SIZE)\n",
    "val_data = dataset_C0(os.path.join(dataset_path,'val.txt'), IMG_SIZE)\n",
    "test_data = dataset_C0(os.path.join(dataset_path,'test.txt'), IMG_SIZE)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 建立feature estractor\n",
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f15024e0-033b-4755-be97-24c6733f1662",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size =  63325\n",
      "val set size =  450\n",
      "test set size =  450\n"
     ]
    }
   ],
   "source": [
    "print('train set size = ',len(train_data))\n",
    "print('val set size = ',len(val_data))\n",
    "print('test set size = ',len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797e37a-c3b2-4e25-9079-66ef164f7a4d",
   "metadata": {},
   "source": [
    "## 利用特徵提取器提取特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad769946-0576-41c3-875e-91656c22f118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_name_list = ['Resnet50','HoG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fa37c-00a6-4b1f-ac34-8f747b0bcda4",
   "metadata": {},
   "source": [
    "### Resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed41072-3587-4dda-a55d-aa02439927b8",
   "metadata": {},
   "source": [
    "* 因為我是使用預訓練的resnet50所以我們只需要利用它提供到進入FC層之前的特徵即可(FC層為一分類器)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d8edcdf-aa21-415b-96f2-25b4097ef3de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 特徵擷取\n",
    "def extract_features(data_loader, model):\n",
    "    features_extractor = nn.Sequential(*list(model.children())[:-1]) #取出最後一層之前的所有層\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    #循環取出所有影像的特徵(進入FC之前的向量)\n",
    "    features, labels = [], []\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        # print(inputs.shape)\n",
    "        with torch.no_grad():\n",
    "            temp = features_extractor(inputs).flatten(start_dim=1)\n",
    "        features.append(temp.cpu().numpy())\n",
    "        labels.append(targets.numpy())\n",
    "    return np.concatenate(features), np.concatenate(labels) #這樣寫是為了方便帶入sklearn的分類器中訓練(要符合固定格式)\n",
    "\n",
    "\n",
    "# # 利用上面的函式提取train set, val set, test set的特徵\n",
    "train_features, train_labels = extract_features(train_loader, model)\n",
    "val_features, val_labels = extract_features(val_loader, model)\n",
    "test_features, test_labels = extract_features(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "414a0fd2-6c07-4561-965f-1bee9cba113c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_name = feature_name_list[0]\n",
    "np.save(os.path.join(dataset_path,'train_features_'+feature_name),train_features)\n",
    "np.save(os.path.join(dataset_path,'train_labels_'+feature_name),train_labels)\n",
    "np.save(os.path.join(dataset_path,'val_features_'+feature_name),val_features)\n",
    "np.save(os.path.join(dataset_path,'val_labels_'+feature_name),val_labels)\n",
    "np.save(os.path.join(dataset_path,'test_features_'+feature_name),test_features)\n",
    "np.save(os.path.join(dataset_path,'test_labels_'+feature_name),test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06b877-fd15-4aaa-9fd7-3fc6f14f6fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40509a20-7055-48bb-a59c-c6a5bfe8c0fc",
   "metadata": {},
   "source": [
    "### HoG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30884e67-404d-40e6-afff-d07af10f4202",
   "metadata": {},
   "source": [
    "# 評估指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5de7827a-9057-4974-b2a7-f92d6b348769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2557833/674719745.py:2: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  top_k_accuracies = sorted_indices==val_labels\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sorted_indices = np.argsort(val_preds_5,axis=1)[::-1][:1]\n",
    "top_k_accuracies = sorted_indices==val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce8eed8b-278d-4e24-84dc-b860ac1e7a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def top_1_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    計算 top 1 accuracy\n",
    "    \n",
    "    參數:\n",
    "    y_true -- 真實標籤 (numpy array= m x 1)\n",
    "    y_pred -- 模型預測機率 (numpy array= m x n)\n",
    "    k -- 前 k 高的機率會被考慮 (int)\n",
    "    m -- 樣本個數\n",
    "    n -- 標籤數量\n",
    "    \n",
    "    回傳:\n",
    "    accuracy -- top k accuracy (float)\n",
    "    \"\"\"\n",
    "    sorted_indices = np.flip(np.argsort(y_pred,axis=1),axis=1) # Descent sorting\n",
    "    pred_top1 = sorted_indices[:,0]\n",
    "    accuracy = np.mean(y_true==pred_top1)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def top_5_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    計算 top 1 accuracy\n",
    "    \n",
    "    參數:\n",
    "    y_true -- 真實標籤 (numpy array= m x 1)\n",
    "    y_pred -- 模型預測機率 (numpy array= m x n)\n",
    "    k -- 前 k 高的機率會被考慮 (int)\n",
    "    m -- 樣本個數\n",
    "    n -- 標籤數量\n",
    "    \n",
    "    回傳:\n",
    "    accuracy -- top k accuracy (float)\n",
    "    \"\"\"\n",
    "    k=2\n",
    "    sorted_indices = np.flip(np.argsort(y_pred,axis=1),axis=1) # Descent sorting\n",
    "    pred_top5 = sorted_indices[:,:5]\n",
    "    num_correct = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if y_true[i] in pred_top5[i,:]:\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / len(y_true)\n",
    "    \n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "633dd59b-b72a-470c-a72e-e6fc8cad5889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy: 0.0\n",
      "Top 2 accuracy: 0.0\n",
      "Top 1 accuracy: 0.3333333333333333\n",
      "Top 5 accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 範例資料\n",
    "y_true = np.array([2,1,0,0,2,1]).reshape(6,1)\n",
    "y_pred = np.array([[0.2, 0.7, 0.1], \n",
    "                   [0.8, 0.3, 0.7], \n",
    "                   [0.8, 0.3, 0.7],\n",
    "                   [0.5, 0.3, 0.7],\n",
    "                   [0.5, 0.3, 0.7],\n",
    "                   [0.1, 0.2, 0.2]])\n",
    "\n",
    "# 計算 top 1 accuracy\n",
    "accuracy = calculate_top_k_accuracy(y_true, y_pred, 1)\n",
    "print(\"Top 1 accuracy:\", accuracy)\n",
    "\n",
    "# 計算 top 2 accuracy\n",
    "accuracy = calculate_top_k_accuracy(y_true, y_pred, 2)\n",
    "print(\"Top 2 accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "top_1 = top_1_acc(y_true, y_pred)\n",
    "print(\"Top 1 accuracy:\", top_1)\n",
    "\n",
    "top_5 = top_5_acc(y_true, y_pred)\n",
    "print(\"Top 5 accuracy:\", top_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a3a14-6dab-4c6c-a27b-5dc32851c7e8",
   "metadata": {},
   "source": [
    "# 訓練和評估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940d79a-530d-4f0b-998d-d1102c021fba",
   "metadata": {},
   "source": [
    "## Features taking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38bdda-88f6-4202-98ea-b1d0701f58f6",
   "metadata": {},
   "source": [
    "'Resnet50','HoG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c26809e-0a41-49cf-89ea-2a0cebb2c470",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet50\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "feature_name = feature_name_list[0]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(feature_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e86c505-3fef-41df-a07c-7f0e559929fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = np.load(os.path.join(dataset_path,'train_features_'+feature_name+'.npy'))\n",
    "train_labels = np.load(os.path.join(dataset_path,'train_labels_'+feature_name+'.npy'))\n",
    "val_features = np.load(os.path.join(dataset_path,'val_features_'+feature_name+'.npy'))\n",
    "val_labels = np.load(os.path.join(dataset_path,'val_labels_'+feature_name+'.npy'))\n",
    "test_features = np.load(os.path.join(dataset_path,'test_features_'+feature_name+'.npy'))\n",
    "test_labels = np.load(os.path.join(dataset_path,'test_labels_'+feature_name+'.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d032ed-3ccd-4422-8915-6db7a43507a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mechine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db1fa5b-47c7-4b78-a23f-c96fa837d6e4",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "abc82432-8a24-42cb-b842-6f53acc927f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_index = np.arange(len(train_features))\n",
    "random.shuffle(train_index)\n",
    "tf = train_features[train_index[:2000],:]\n",
    "tl = train_labels[train_index[:2000]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b1fd70f-2b03-419e-a75d-8f8ba6e67e71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = val_labels.copy()\n",
    "le.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0daa9cce-97e9-4ec6-b1b3-fc8d2e534256",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2557833/2499075316.py:17: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  top_k_accuracies = sorted_indices==y_true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_1_acc(val_labels,val_preds_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61d2ae45-3059-493a-8678-86165dd818d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yancheng/miniconda3/envs/DL/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_depth = 3\n",
    "\n",
    "# 建立AdaBoost分類器\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=best_depth)\n",
    ")\n",
    "\n",
    "# 訓練AdaBoost分類器\n",
    "# adaboost.fit(train_features, train_labels)\n",
    "adaboost.fit(tf, tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca2bc0f6-996a-4e9a-af0b-13b96d498ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_top_k_accuracy(val_preds_5, val_labels,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ad6fc66-e162-49fd-8f29-5a2e8384c9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 50)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m val_preds_5 \u001b[38;5;241m=\u001b[39m adaboost\u001b[38;5;241m.\u001b[39mpredict_proba(val_features)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(val_preds_5\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop1 Accuracy on validation set: \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m      \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcalculate_top_k_accuracy(val_preds,\u001b[38;5;250m \u001b[39mval_labels,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop5 Accuracy on validation set: \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m      \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcalculate_top_k_accuracy(val_preds_5,\u001b[38;5;250m \u001b[39mval_labels,\u001b[38;5;241m5\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 32\u001b[0m, in \u001b[0;36mcalculate_top_k_accuracy\u001b[0;34m(y_true, y_pred, k)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m計算平均 top k accuracy\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03maccuracy -- 平均 top k accuracy (float)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m top_k_accuracies \u001b[38;5;241m=\u001b[39m [top_k_accuracy(y_true[i], y_pred[i], k) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]\n\u001b[1;32m     33\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(top_k_accuracies)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "Cell \u001b[0;32mIn[38], line 32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m計算平均 top k accuracy\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03maccuracy -- 平均 top k accuracy (float)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m top_k_accuracies \u001b[38;5;241m=\u001b[39m [\u001b[43mtop_k_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]\n\u001b[1;32m     33\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(top_k_accuracies)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "Cell \u001b[0;32mIn[38], line 14\u001b[0m, in \u001b[0;36mtop_k_accuracy\u001b[0;34m(y_true, y_pred, k)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m計算 top k accuracy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03maccuracy -- top k accuracy (float)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(y_pred)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:k]\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(\u001b[43my_true\u001b[49m\u001b[43m[\u001b[49m\u001b[43msorted_indices\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "# val\n",
    "\n",
    "val_preds = adaboost.predict(val_features)\n",
    "val_preds_5 = adaboost.predict_proba(val_features)\n",
    "print(val_preds_5.shape)\n",
    "print(f'Top1 Accuracy on validation set: \\\n",
    "      {calculate_top_k_accuracy(val_preds, val_labels,1):.4f}')\n",
    "print(f'Top5 Accuracy on validation set: \\\n",
    "      {calculate_top_k_accuracy(val_preds_5, val_labels,5):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be045ab-22b0-4d5c-b619-4899cb605d72",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea1b55-50cf-4556-8899-aba2a87ab358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "483cb15c-9d84-4335-84cd-d252ee4a2479",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fae0fb-d11b-4902-ab4b-9d3ab8b422ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5877cc9-3789-4fd6-b60b-904ea28874b4",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5bb16-737e-4bfb-b5a6-e4315c3cf416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8b0853f-f119-4635-bcb5-0c574de2c9dc",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4d4f8-56d1-4114-93bf-094c14191096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "313c3b66-4a54-48d5-bff7-258b9cde470e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bec51ab-c028-4e3a-9efd-8b036387c420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 將提取的特徵轉為DataLoader形式\n",
    "def feature_loader(features,labels,BATCH_SIZE):\n",
    "    X = torch.tensor(features, dtype=torch.float32)\n",
    "    y = torch.tensor(labels, dtype=torch.float32).reshape(-1, 1)\n",
    "    loader = torch.utils.data.DataLoader(list(zip(X,y)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "train_feature_loader = feature_loader(train_features,train_labels,BATCH_SIZE)\n",
    "val_feature_loader = feature_loader(val_features,val_labels,BATCH_SIZE)\n",
    "test_feature_loader = feature_loader(test_features,test_labels,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f785a-00b5-4d77-aaf3-026de8339340",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19d7e6f1-65a5-4ff5-95dc-04bb588c4492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 2048]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_feature_loader, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     21\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 22\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# y_pred = torch.argmax(softmax(y_logit, dim=1), dim=1) # top-1\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# loss = loss_fcn(y_logit, y.to(torch.int64))\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# train_loss += loss\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# train_acc += accuracy_fn(y, y_pred)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 2048]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# model = CNN(num_classes=50).to(device)\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "EPOCH = 30\n",
    "train_loss_list = []\n",
    "train_top1_list = []\n",
    "train_top5_list = []\n",
    "val_loss_list = []\n",
    "val_top1_list = []\n",
    "val_top5_list = []\n",
    "for epoch in range(EPOCH):\n",
    "    # print(f\"Epoch: {epoch + 1}\")\n",
    "    ### training\n",
    "    model.train()\n",
    "    train_loss, train_top1, train_top5 = 0, 0, 0\n",
    "    for batch, (X, y) in enumerate(train_feature_loader, start=1):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        output = model(X)\n",
    "        # y_pred = torch.argmax(softmax(y_logit, dim=1), dim=1) # top-1\n",
    "        # loss = loss_fcn(y_logit, y.to(torch.int64))\n",
    "        # train_loss += loss\n",
    "        # train_acc += accuracy_fn(y, y_pred)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        break\n",
    "        \n",
    "        # if batch % 400 == 0:\n",
    "            # print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "        \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_top1_list.append(train_top1)\n",
    "    train_top5_list.append(train_top5)\n",
    "    \n",
    "\n",
    "    ### validation\n",
    "    model.eval()\n",
    "    val_loss, val_top1, val_top5 = 0, 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for X, y in val_feature_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            val_logit = model(X)\n",
    "            val_pred = torch.argmax(softmax(val_logit, dim=1), dim=1)\n",
    "\n",
    "            val_loss += loss_fcn(val_logit, y.to(torch.int64))\n",
    "            val_acc += accuracy_fn(y, val_pred)\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_acc /= len(val_dataloader)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_top1_list.append(val_top1)\n",
    "        val_top5_list.append(val_top5)\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.5f}, Train acc: {train_acc:.2f}% | Validation loss: {val_loss:.5f}, Validation acc: {val_acc:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
